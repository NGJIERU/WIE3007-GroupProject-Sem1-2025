{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# STEP 5 â€” EVALUATION (FOUNDATION-FIRST)\n",
                "\n",
                "## ðŸŽ¯ Objective\n",
                "This notebook serves as the **standardized evaluation hub** for the project. it will:\n",
                "1.  Define the **metrics** used for all models.\n",
                "2.  Maintain a **consistent results table**.\n",
                "3.  Compare models as they are developed."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Standard Evaluation Metrics\n",
                "\n",
                "> **Evaluation Metrics**\n",
                "> For classification, all models are evaluated using **Accuracy**, **F1-score**, and **ROC-AUC** to ensure fair comparison across algorithms.\n",
                "\n",
                "These 3 metrics are:\n",
                "*   **Required** by the assignment.\n",
                "*   **Suitable** for our potentially imbalanced e-commerce data.\n",
                "*   **Common** across different model types (Tree, NB, etc.)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schema_title",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ Results Schema\n",
                "\n",
                "We define a global list `results` to store metrics for all models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "schema_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global results list\n",
                "results = []\n",
                "\n",
                "def add_result(model_name, accuracy, f1, roc_auc):\n",
                "    \"\"\"\n",
                "    Adds a model's performance metrics to the global results list.\n",
                "    \"\"\"\n",
                "    results.append({\n",
                "        \"model\": model_name,\n",
                "        \"accuracy\": accuracy,\n",
                "        \"f1_score\": f1,\n",
                "        \"roc_auc\": roc_auc\n",
                "    })\n",
                "    print(f\"âœ… Added results for: {model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "baseline_title",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Baseline Model: Decision Tree (Person A)\n",
                "\n",
                "We load the data, re-train the baseline model (integration test), and log its performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
                "\n",
                "# 1. Load Data\n",
                "X = pd.read_csv(\"../data/features.csv\")\n",
                "y = pd.read_csv(\"../data/target.csv\").squeeze()\n",
                "\n",
                "# 2. Stratified Split (Must match Modeling notebook)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# 3. Train Baseline\n",
                "dt = DecisionTreeClassifier(random_state=42)\n",
                "dt.fit(X_train, y_train)\n",
                "\n",
                "# 4. Compute Metrics\n",
                "y_pred = dt.predict(X_test)\n",
                "y_prob = dt.predict_proba(X_test)[:, 1]\n",
                "\n",
                "acc = accuracy_score(y_test, y_pred)\n",
                "f1 = f1_score(y_test, y_pred)\n",
                "roc = roc_auc_score(y_test, y_prob)\n",
                "\n",
                "# 5. Add to Results\n",
                "add_result(\"Decision Tree\", acc, f1, roc)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results_header",
            "metadata": {},
            "source": [
                "## 4ï¸âƒ£ Results Comparison\n",
                "\n",
                "We visualize the performance of all models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "results_table",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create DataFrame\n",
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "# Display Table\n",
                "print(\"Execution Successful. Current Results:\")\n",
                "display(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "results_plot",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot Accuracy and F1 Score\n",
                "if not results_df.empty:\n",
                "    results_df.set_index(\"model\")[[\"accuracy\", \"f1_score\"]].plot(kind=\"bar\", figsize=(10, 5))\n",
                "    plt.title(\"Model Comparison: Accuracy vs F1 Score\")\n",
                "    plt.ylabel(\"Score\")\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.ylim(0, 1)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No results to plot.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "insight",
            "metadata": {},
            "source": [
                "## 5ï¸âƒ£ Baseline Evaluation Insight\n",
                "\n",
                "> **Baseline Evaluation Insight**\n",
                "> The Decision Tree baseline achieves reasonable performance above random guessing, validating the suitability of the dataset and engineered features for predictive modelling. More advanced models are expected to improve performance and generalisation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "note",
            "metadata": {},
            "source": [
                "## 6ï¸âƒ£ Note to Model Owners\n",
                "> **Note to Model Owners**\n",
                "> Each model owner must provide accuracy, F1-score, and ROC-AUC computed on the same test split (random_state=42, stratify=y) and append results using the `add_result()` function."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ai_usage",
            "metadata": {},
            "source": [
                "## 7ï¸âƒ£ AI-Assisted Interpretation\n",
                "> **AI-Assisted Interpretation**\n",
                "> A Large Language Model was used to assist in summarising observed performance trends and interpreting model behaviour once all models are evaluated."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
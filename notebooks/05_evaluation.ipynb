{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# STEP 5 ‚Äî EVALUATION (FOUNDATION-FIRST)\n",
                "\n",
                "## üéØ Objective\n",
                "This notebook serves as the **standardized evaluation hub** for the project. it will:\n",
                "1.  Define the **metrics** used for all models.\n",
                "2.  Maintain a **consistent results table**.\n",
                "3.  Compare models as they are developed."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Standard Evaluation Metrics\n",
                "\n",
                "> **Evaluation Metrics**\n",
                "> For classification, all models are evaluated using **Accuracy**, **F1-score**, and **ROC-AUC** to ensure fair comparison across algorithms.\n",
                "\n",
                "These 3 metrics are:\n",
                "*   **Required** by the assignment.\n",
                "*   **Suitable** for our potentially imbalanced e-commerce data.\n",
                "*   **Common** across different model types (Tree, NB, etc.)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schema_title",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Results Schema\n",
                "\n",
                "We define a global list `results` to store metrics for all models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "schema_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global results list\n",
                "results = []\n",
                "\n",
                "def add_result(model_name, accuracy, f1, roc_auc):\n",
                "    \"\"\"\n",
                "    Adds a model's performance metrics to the global results list.\n",
                "    \"\"\"\n",
                "    results.append({\n",
                "        \"model\": model_name,\n",
                "        \"accuracy\": accuracy,\n",
                "        \"f1_score\": f1,\n",
                "        \"roc_auc\": roc_auc\n",
                "    })\n",
                "    print(f\"‚úÖ Added results for: {model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "baseline_title",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Baseline Model: Decision Tree (Person A)\n",
                "\n",
                "We load the data, re-train the baseline model (integration test), and log its performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
                "\n",
                "# 1. Load Data\n",
                "X = pd.read_csv(\"../data/features.csv\")\n",
                "y = pd.read_csv(\"../data/target.csv\").squeeze()\n",
                "\n",
                "# 2. Stratified Split (Must match Modeling notebook)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# 3. Train Baseline\n",
                "dt = DecisionTreeClassifier(random_state=42)\n",
                "dt.fit(X_train, y_train)\n",
                "\n",
                "# 4. Compute Metrics\n",
                "y_pred = dt.predict(X_test)\n",
                "y_prob = dt.predict_proba(X_test)[:, 1]\n",
                "\n",
                "acc = accuracy_score(y_test, y_pred)\n",
                "f1 = f1_score(y_test, y_pred)\n",
                "roc = roc_auc_score(y_test, y_prob)\n",
                "\n",
                "# 5. Add to Results\n",
                "add_result(\"Decision Tree\", acc, f1, roc)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results_header",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Results Comparison\n",
                "\n",
                "We visualize the performance of all models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "results_table",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create DataFrame\n",
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "# Display Table\n",
                "print(\"Execution Successful. Current Results:\")\n",
                "display(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "results_plot",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot Accuracy and F1 Score\n",
                "if not results_df.empty:\n",
                "    results_df.set_index(\"model\")[[\"accuracy\", \"f1_score\"]].plot(kind=\"bar\", figsize=(10, 5))\n",
                "    plt.title(\"Model Comparison: Accuracy vs F1 Score\")\n",
                "    plt.ylabel(\"Score\")\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.ylim(0, 1)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No results to plot.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
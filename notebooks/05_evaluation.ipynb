{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# STEP 5 â€” EVALUATION (FOUNDATION-FIRST)\n",
                "\n",
                "## ðŸŽ¯ Objective\n",
                "This notebook serves as the **standardized evaluation hub** for the project. it will:\n",
                "1.  Define the **metrics** used for all models.\n",
                "2.  Maintain a **consistent results table**.\n",
                "3.  Compare models as they are developed."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Standard Evaluation Metrics\n",
                "\n",
                "> **Evaluation Metrics**\n",
                "> For classification, all models are evaluated using **Accuracy**, **F1-score**, and **ROC-AUC** to ensure fair comparison across algorithms.\n",
                "\n",
                "These 3 metrics are:\n",
                "*   **Required** by the assignment.\n",
                "*   **Suitable** for our potentially imbalanced e-commerce data.\n",
                "*   **Common** across different model types (Tree, NB, etc.)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
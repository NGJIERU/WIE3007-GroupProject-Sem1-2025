{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# STEP 5 ‚Äî EVALUATION (FOUNDATION-FIRST)\n",
                "\n",
                "## üéØ Objective\n",
                "This notebook serves as the **standardized evaluation hub** for the project. it will:\n",
                "1.  Define the **metrics** used for all models.\n",
                "2.  Maintain a **consistent results table**.\n",
                "3.  Compare models as they are developed."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Standard Evaluation Metrics\n",
                "\n",
                "> **Evaluation Metrics**\n",
                "> For classification, all models are evaluated using **Accuracy**, **F1-score**, and **ROC-AUC** to ensure fair comparison across algorithms.\n",
                "\n",
                "These 3 metrics are:\n",
                "*   **Required** by the assignment.\n",
                "*   **Suitable** for our potentially imbalanced e-commerce data.\n",
                "*   **Common** across different model types (Tree, NB, etc.)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schema_title",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Results Schema\n",
                "\n",
                "We define a global list `results` to store metrics for all models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "schema_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global results list\n",
                "results = []\n",
                "\n",
                "def add_result(model_name, accuracy, f1, roc_auc):\n",
                "    \"\"\"\n",
                "    Adds a model's performance metrics to the global results list.\n",
                "    \"\"\"\n",
                "    results.append({\n",
                "        \"model\": model_name,\n",
                "        \"accuracy\": accuracy,\n",
                "        \"f1_score\": f1,\n",
                "        \"roc_auc\": roc_auc\n",
                "    })\n",
                "    print(f\"‚úÖ Added results for: {model_name}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
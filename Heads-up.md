# ðŸš€ Project Handoff & Quick Start Guide

**TO:** Team Members (Backend / Models)
**FROM:** Person A (Data Pipeline & Baseline)
**DATE:** 2023-12-23
**STATUS:** âœ… Pipeline Fixed & Verified (~90% Baseline Accuracy)

---

## 1. What I Have Done (The "Master Fix")
I have completely refactored the project to fix the **data leakage** issue that was giving us fake 100% accuracy. The project is now academically defensible and ready for your models.

*   **Fixed Data Leakage**: Removed `days_since_last_purchase` from the training set. It was cheating.
*   **Realistic Logic**: `Churn` is now probabilistic (based on risk scores), not deterministic.
*   **Noise Injection**: I added "Karen" customers (active but complainers) so models can't just memorize rules.
*   **Validated Baseline**: The Decision Tree now gets **~90% Accuracy** (instead of 100%), which is a strong, realistic signal.

---

## 2. Your Mission: Pick a Model
We need to implement the advanced models listed in our report structure. **Please claim one:**

| Model | Complexity | Goal | Note |
| :--- | :--- | :--- | :--- |
| **Logistic Regression** | Low | **Interpretability** | Will likely perform *worse* (~80%) because our data rules are non-linear. This proves we need ML! |
| **Random Forest** | Medium | **Stability** | Should be robust against the noise I injected. Expect ~91-92%. |
| **XGBoost** | High | **Performance** | The "Champion" model. Should find the hidden age-income interactions. |
| **MLP (Neural Network)** | High | **Complexity Demo** | Show that Deep Learning is overkill (or surprising) for tabular data. |

---

## 3. How to Start (Git & Environment)

### **Step A: Git Rules (CRITICAL âš ï¸)**
**RULE 1:** ðŸ›‘ **DO NOT edit `main` directly.**
**RULE 2:** Always create a new branch for your model.
**RULE 3:** Use a Pull Request (PR) to merge your code back.

**Workflow:**
1.  **Sync First:**
    ```bash
    git checkout main
    git pull origin main  # Get my latest fixes
    ```
2.  **Create Your Branch:**
    ```bash
    git checkout -b model/your-model-name
    ```
3.  **Work & Push:**
    ```bash
    # ... do your work ...
    git add .
    git commit -m "Added Random Forest model"
    git push -u origin model/your-model-name
    ```
4.  **Merge:** Go to GitHub and create a **Pull Request** to merge into `main`.

### **Step B: Environment**
I added a `venv` setup.
```bash
source venv/bin/activate
pip install -r requirements.txt
jupyter notebook
```

---

## 4. Where is the Data? (Do NOT preprocess again)
I have already saved the clean, engineered data for you. **Load it exactly like this in your notebook:**

```python
import pandas as pd
from sklearn.model_selection import train_test_split

# 1. Load Pre-processed Data
X = pd.read_csv("../data/features.csv")
y = pd.read_csv("../data/target.csv").squeeze()

# 2. âš ï¸ DROP LEAKAGE COLUMN (Required!)
if 'days_since_last_purchase' in X.columns:
    X = X.drop(columns=['days_since_last_purchase'])

# 3. Standard Split (Must use random_state=42 for fairness)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

**Key Features You Will See:**
*   `sentiment_score`: Strongest predictor (derived from reviews).
*   `risk_score`: A helper score, often redundant but good for explainability.
*   `spend_ratio`: New economic feature I added (Estimated Spend / Income).

---

## 5. How to Evaluate (Standardized)
We must use the **exact same metrics** to compare apples-to-apples.

In your notebook, please add your final results to the shared table schema:

```python
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# ... train your model ...
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # Needed for AUC

# Print for the report
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
print(f"ROC-AUC:  {roc_auc_score(y_test, y_prob):.4f}")
```

### **What to write in your Report.md?**
*   **If your model > 90%**: "The model successfully captured non-linear interactions between demographic clusters (`age` vs `income`) that the Decision Tree missed."
*   **If your model < 90%**: "The linear nature of this model limited its ability to separate the probabilistic 'Karen' noise from true churners."

---

**Let's crush this presentation! ðŸš€**